:PROPERTIES:
:ID:       f040dd01-cc36-4173-aa18-4035718e7879
:END:
#+title: Kaggle: Intro to Deep Learning
#+filetags: :MachineLearning:AI:



* Neurons

Deep Learning - an approach to machine learning characterized by deep stacks of computations..
This depth is what enable deep learning models to disentangle and interpret complex and hierarchical problems.

Nerual Networks are recognized as the defining model of deep learning. They are composed of *Neurons*, where each neuron performs a single computation.
Neurons, or /units/, with one input are defined by the equation $y=wx+b$, where the input is x. Its connection to the neuron has a multiplicative weight /w/.
A neuron learns by modifying the weights attached to each neuron. /b/ represents the neurons bias - which is an additive constant that allows the neuron to modify its output independent of its input. 

We can expand our neurons to take more inputs as well, in this case we would add multiple weights and keep our individual bias. This would create an equation that looked like this: $y=w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2}+b$ where each x is our input, like before, and each w is its corresponding weight.

In python, we can use a tensorflow module called /keras/ to create deep learning models.

#+begin_src python
  from tensorflow import keras
  from tensorflow.keras import layers

  # Create a model with 1 linear unit
  model=keras.Sequential([
  layers.Dense(units=1, input_shape=[3])
  ])
#+end_src

Here, =units= defines how many outputs we want.
=input_shape= specifies the input dimensions, setting =input_shape = [3]= ensures the model will take input from 3 features.

* Coding Exercise
Set up Plotting
#+begin_src python :results verbatim :session Kaggle-DL
  import matplotlib.pyplot as plt

  plt.style.use('seaborn-whitegrid')
  plt.rc('figure', autolayout=True)
  plt.rc('axes', labelweight='bold', labelsize='large', titleweight='bold', titlesize=18, titlepad=10)
#+end_src

#+RESULTS:

#+begin_src python :results verbatim :session Kaggle-DL
  import pandas as pd

  red_wine=pd.read_csv('/home/csj7701/roam/References/Kaggle-WineData.csv')
  red_wine.head()
#+end_src

#+RESULTS:
:    fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality
: 0            7.4              0.70         0.00  ...       0.56      9.4        5
: 1            7.8              0.88         0.00  ...       0.68      9.8        5
: 2            7.8              0.76         0.04  ...       0.65      9.8        5
: 3           11.2              0.28         0.56  ...       0.58      9.8        6
: 4            7.4              0.70         0.00  ...       0.56      9.4        5
: 
: [5 rows x 12 columns]

Get the /shape/ - number of rows and columns
#+begin_src python :results verbatim :session Kaggle-DL
  red_wine.shape
#+end_src

#+RESULTS:
: (1599, 12)

Defining a Linear Model, where the target is quality and the remaining columns are features.
#+begin_src python :results verbatim :session Kaggle-DL
  from tensorflow import keras
  from tensorflow.keras import layers

  model=keras.Sequential([
  layers.Dense(units=1, input_shape=[11])
  ])

#+end_src

#+RESULTS:
: None

Here, we know that there are 12 total columns, and since we are looking for quality, the remaining 11 will be inputs to our model. Thus, we have to ensure that the model takes 11 inputs.


Keras represents weight with something called a *Tensor*, which is effectively an array that is treated specially by the tensorflow module. The main difference is that tensors are compatible with GPU and TPU acceleration.
A models weights are stored in its =weights= attribute as a list of tensors.
#+begin_src python :results verbatim :session Kaggle-DL
  model.weights
#+end_src

#+RESULTS:
#+begin_example
[<tf.Variable 'dense/kernel:0' shape=(11, 1) dtype=float32, numpy=
array([[ 0.26382798],
       [-0.49166387],
       [-0.38165185],
       [ 0.00448996],
       [ 0.42288262],
       [ 0.32784933],
       [-0.2744874 ],
       [-0.5374677 ],
       [-0.44155195],
       [ 0.26299298],
       [ 0.32698447]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
#+end_example

Result should be:

Weights
<tf.Variable 'dense/kernel:0' shape=(11, 1) dtype=float32, numpy =
array([[-0.5920056 ],
       [-0.4777234 ],
       [-0.16399086],
       [ 0.3738181 ],
       [-0.31306443],
       [-0.3832509 ],
       [ 0.52106875],
       [ 0.14226216],
       [-0.08233064],
       [ 0.10293877],
       [-0.1660847 ]], dtype=float32)>

Bias
<tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>

We can see that there is one weight per input here, plus a single bias.
There doesn't seem to be a pattern to the weight value - this is because the model is untrained. An untrained model will produce random weights, and a bias of 0.0
The model /learns/ by finding better values for these weights.

** Supplemental - plotting
The examples we work with will primarily be /regression/ models - the goal is to predict some numeric target. These are essentially "curve-fitting" problems, where we try to find a curve that fits the data as closely as possible.
Linear models. like the one we created above, produce a "linear" curve (a line).
#+begin_src python :output verbatim image
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import layers
  import matplotlib.pyplot as plt

  model=keras.Sequential([layers.Dense(1, input_shape=[1]),])

  x=tf.linspace(-1.0, 1.0, 100)
  y=model.predict(x)

  plt.figure(dpi=100)
  plt.plot(x,y,'k')
  plt.xlim(-1,1)
  plt.ylim(-1,1)
  plt.xlabel("Input: x")
  plt.ylabel("Target y")
  w, b = model.weights
  plt.title("Weight: {:0.2f}\nBias: {:0.2f}".format(w[0][0], b[0]))
  #plt.show()

#+end_src

#+RESULTS:
#+ATTR_LATEX: :caption \bicaption{---}
[[file:/home/csj7701/roam/Attachments/Kaggle-Deep-Learning-1.png]]
* Deep Neural Networks
We will be adding hidden layers to network, which will allow us to explore more complex relationships.

Neural Networks typically organize their neurons into *layers*.
When we collect sets of linear units which share a common set of inputs, we get a *dense layer*.

If each layer in a network performs a simple operation, as we add more and more layers, we can achieve more and more complexity.

Since dense layers are comprised of linear operations, placing two of them together with nothing in between performs no differently from a single dense layer.
Because of this, we need to add a /nonlinear/ component - this is called the *activation function*.

An activation function is simply a function that we apply to each of the layers outputs (also called its *activations*).
The most common is the /rectifier function/, =max(0,x)=.
When we attach the rectifier to a linear unit,we get something known as the rectified linear unit, or ReLU.
Thus, the rectifier function is often known as the "ReLU function".

#+ATTR_LATEX: :caption \bicaption{---}
[[file:/home/csj7701/roam/Attachments/Kaggle-Deep-Learning-2.png]]


The layers between the input and output are often called "hidden", since we can't directly see their output.
The final (output) layer is always a linear unit when we are approaching a regression problem (solving for some arbitrary number).

#+begin_src python :results verbatim

   from tensorflow import keras
   from tensorflow.keras import layers

   model=keras.Sequential([
  # Hidden ReLU Layers
  layers.Dense(units=4, activation='relu', input_shape=[2]),
  layers.Dense(units=3, activation='relu'),
  # Linear Output Layer
  layers.Dense(units=1),
  ])

#+end_src

#+RESULTS:
: None

* Coding Exercise

#+begin_src python :results verbatim :session Kaggle-DL-2
  import tensorflow as tf
  import matplotlib.pyplot as plt
  import pandas as pd
  from tensorflow import keras
  from tensorflow.keras import layers

  # plt.style.use('seaborn-whitegrid')
  plt.rc('figure', autolayout=True)
  plt.rc('axes',labelweight='bold',labelsize='large',titleweight='bold',titlesize='18',titlepad=10)

  concrete=pd.read_csv('/home/csj7701/roam/References/Kaggle-ConcreteData.csv')
  concrete.head()
#+end_src

#+RESULTS:
:    Cement  BlastFurnaceSlag  FlyAsh  ...  FineAggregate  Age  CompressiveStrength
: 0   540.0               0.0     0.0  ...          676.0   28                79.99
: 1   540.0               0.0     0.0  ...          676.0   28                61.89
: 2   332.5             142.5     0.0  ...          594.0  270                40.27
: 3   332.5             142.5     0.0  ...          594.0  365                41.05
: 4   198.6             132.4     0.0  ...          825.5  360                44.30
: 
: [5 rows x 9 columns]

We want to target this model for 'CompressiveStrength'. There are 8 other columns, so we will set our input shape to 8 when we define our model.
We will create a model with 3 hidden layers, each with 512 units and the ReLU activation. 
#+begin_src python :results verbatim :session Kaggle-DL-2
  model=keras.Sequential([
  layers.Dense(units=512, activation='relu', input_shape=[8]),
  layers.Dense(units=512, activation='relu'),
  layers.Dense(units=512, activation='relu'),
  layers.Dense(units=1)])
#+end_src

We can also define activation layers as their own layers - this is useful if we want to place anythin between

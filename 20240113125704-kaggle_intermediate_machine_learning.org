:PROPERTIES:
:ID:       34108a3d-efff-43a3-9bab-e9b5fbb0fab8
:END:
#+title: Kaggle: Intermediate Machine Learning

* Code Exercise

#+begin_src python :results verbatim :session Kaggle-IML

  import os
  if not os.path.exists("/home/csj7701/roam/References/Kaggle-Train.csv"):
      print("File not found")

  import pandas as pd
  from sklearn.model_selection import train_test_split

  X_full=pd.read_csv("/home/csj7701/roam/References/Kaggle-Train.csv")
  X_test_full=pd.read_csv("/home/csj7701/roam/References/Kaggle-Test.csv")

  y=X_full.SalePrice
  features=['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
  X=X_full[features].copy()
  X_test=X_test_full[features].copy()

  X_train, X_valid, y_train, y_valid=train_test_split(X,y,train_size=0.8, test_size=0.2,random_state=0)

  X_train.head()

#+end_src

#+RESULTS:
:      LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  TotRmsAbvGrd
: 618    11694       2007      1828         0         2             3             9
: 870     6600       1962       894         0         1             2             5
: 92     13360       1921       964         0         1             2             5
: 817    13265       2002      1689         0         2             3             7
: 302    13704       2001      1541         0         2             3             6

#+begin_src python :results verbatim :session Kaggle-IML

  from sklearn.ensemble import RandomForestRegressor

  model_1=RandomForestRegressor(n_estimators=50, random_state=0)
  model_2=RandomForestRegressor(n_estimators=100, random_state=0)
  model_3=RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)
  model_4=RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)
  model_5=RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)

  models=[model_1, model_2, model_3, model_4, model_5]
#+end_src

#+RESULTS:
: None

#+begin_src python :results verbatim :session Kaggle-IML

  from sklearn.metrics import mean_absolute_error

  def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):
      model.fit(X_t, y_t)
      preds=model.predict(X_v)
      return mean_absolute_error(y_v, preds)

  result=""
  for i in range(0, len(models)):
      mae=score_model(models[i])
      new_result="Model {} MAE: {}\n".format(i+1, mae)
      result+=new_result
  result

#+end_src

#+RESULTS:
: Model 1 MAE: 24015.492818003917
: Model 2 MAE: 23740.979228636657
: Model 3 MAE: 23528.78421232877
: Model 4 MAE: 23996.676789668687
: Model 5 MAE: 23706.672864217904


Here, we can see that the best performing model is model_3 with the lowest MAE.

#+begin_src python :results verbatim :session Kaggle-IML

  my_model=model_3

  my_model.fit(X,y)

  preds_test=my_model.predict(X_test)
  output=pd.DataFrame({'Id':X_test.index,'SalePrice':preds_test})
  output

#+end_src

#+RESULTS:
#+begin_example
        Id  SalePrice
0        0  119433.08
1        1  158367.50
2        2  185351.21
3        3  178343.12
4        4  192898.29
...    ...        ...
1454  1454   86155.00
1455  1455   89050.00
1456  1456  156296.92
1457  1457  132232.50
1458  1458  230870.60

[1459 rows x 2 columns]
#+end_example

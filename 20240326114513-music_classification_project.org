:PROPERTIES:
:ID:       fa657b96-e628-4cc3-a4e6-beae92d669a3
:VISIBILITY: folded
:END:
#+title: Music Classification Project
#+category: Music Classification Project
#+filetags: :DSP:Project:
#+STARTUP: shrink
#+EXPORT_FILE_NAME: /home/csj7701/Projects/Music-Classification/Class-Documents/ProjectOverview.pdf
* Project Description
Using signal analysis techniques learned in Digital Signal Processing to recognize music and generate similar playlists.
** Goals
- Learn about different approaches to music similarity scoring and recommendations; derive main results
- Develop program that determines how similar songs are to each other; make code modular and able to accept new music files.
- Build a recommender system that runs on a loop playing audio files most similar to each other.
- Incorporate a live music feature into the program, allowing a user to record a live snippet of audio. System will check if audio file is already present.
- Determine how the system should work in production and what resources are required vs number of users.

** Advanced Goals
- extend code base to incorporate another recommendation system based off a specific feature or features of the music files (dance tempo, etc)
- Improve your program, eg: faster runtime, recording robustness to noise, lower false positive rate, etc.
- Build a simple ML based recommender system and compare it to your other results.

* Project Components

** Characterize Song
We need to choose a method with which to represent a song's characteristics.
Right now, it looks as though we have 4 logical choices with which we could proceed:
- Frequency Distribution (This is found with the FFT and prob theory)
- MFCC (This represents the general "texture" of the song - still a little unclear what that actually means)
- Rhythm (Would need to find a more generalized method of representing this - [[*Fluctuation Patterns][Fluctuation Patterns]] and [[*"Penny" Features]["Penny" Features]] could be promising)
- Melody (Chromagrams)

The best decision would likely be to combine several of these options - perhaps we calculate all of them and assign each a "weight" value to indicate how much we care about similarity in each category?

Whatever method we choose should represent the track in some standardized format - perhaps a sequence?
/Note/ - would the sequence need to maintain a common length?

Song 1 = [(12,14,15),(25.5 bpm), (12,13,14), (14,15,16)]

After review, we now use a segmenting technique recommended by the shazam article.
We compute the FFT of a song, then use an array of frequency breakpoints to segment the FFT base on lows, mids, highs, etc.
Since the FFT is normalized, we have to do some math using sampling rate to convert the Hz frequencies to values on the X axis of the FFT.
We find the bin with the highest magnitude within each segment, returning a complex number representing that bin.
Need to do more research on precisely /what/ this complex number represents. From the complex number we create a point in polar coordinates.
We end up with an array of polar coordinates for each song.

** Compare Songs
Before we can generate recommendations from a large collection of music, we must first be able to calculate how similar two tracks are to each other.

At an early stage, this will involve taking two songs as an input, calculating our chosen characteristics, and comparing the two results.
It will make much more sense to compare the /distribution/ of each result, rather than attempting to compare the raw values. This likely means that the length of the original sequence for each result is arbitrary.
If we choose to base our comparison off of multiple characteristics, we will need to compare the difference between each characteristic, then we have a choice.
We could either multiply each difference by a scale factor that represents the weight we choose for a characteristic, OR we could store all the values in an array.
/Note/ - whichever we choose, we will most likely want to arrive at a single scalar value to represent similarity (value will reside in a range - i.e. will be normalized)

Using the array of polar coordinates described above, we can find a basic similarity.
We know that polar coordinates simply represent a point. We can find the "Euclidean Distance", or linear space between 2 points, and use that.
For each song, we know that the arrays will be the same size, since we use the same breakpoints to divide them all.
This means that we can find the distance between the polar coordinates at each index for two songs.
This will create an array of scalar values. Simplest solution to convert this to a single comparison factor seems to be taking the average of the array.

** Apply Comparisons
The end goal of this project is to recommend a "playlist" of songs that are similar to some starting point.
Presumably, we will only be able to work with songs for which we have access to .mp3 or .wav files for (I can supply, I've got a moderate collection of local music).

I can see two logical ways to approach this.

1. Live scanning
   User will input a song (list of songs?) and the system will calculate its list of characteristics.
   System will then calculate list of characteristics for every other song in a given directory.
   System will skip/break loop if any of the newly scanned characteristics are further than a pre-defined threshold.
   Will progressively gather list of songs that are not outside that threshold, and this will become the playlist.
   Pros:
   - Data is in standard format
   - No need to compress data
   Cons:
   - Will likely be slow
   - Heavily resource intensive - probably won't be able to run on issued laptops.

2. Compression and Database
   User will point system at a directory containing music files. System will scan each file, calculate its characteristic values.

   Depending on the format in which we output characteristic values, this step may differ.
   The system should somehow compress/hash the characteristic values in order to store them in a database (I don't believe there's an efficient way to store array data types in a database, unless we convert the array to a string... which is dumb.)
   We can figure out database structure later if we choose this route.
   Alternatively, there may be a more efficient method than a database for storing this type of data - I'm unsure.

   User will input a song, system will calculate its list of characteristics.
   System will query database to find values within the threshold compared to the input song.
   Output from this query will become the playlist.
   Pros:
   - Should be significantly faster, depending on data structure
   - Will scale easily for larger collections of music
  Cons:
  - Will need to figure out some efficient database scheme to store characteristic data
  - Will likely need to convert raw characteristic data in order to store in database

This step will likely require some sort of FOR loop to check multiple items and calculate successive similarities - it's worth looking into a way to optimize this "stepping" rather than simply iterating through every single item.
Look into [[*LSH][LSH]] and [[*Ball Trees][Ball Trees]].

We ended up using 2 different handmade solutions.
I created a dictionary to store the raw data - keys are song basenames (may want to move to filepaths later) and values are the polar coordinate arrays.
I created a list to store the comparison data - the list stores 3 element arrays, where the first two values are the songnames being compared, and the third is the distance. In order to retrieve a songs comparison, we simply search for the songs name to be present in a subarray, grabbing the other songname and value when we do.

* References
:PROPERTIES:
:VISIBILITY: children
:END:

| Title                                                                                          | Link | Abstract | Conclusion | Thorough | Done |
|------------------------------------------------------------------------------------------------+------+----------+------------+----------+------|
| <50>                                                                                           | <5>  | <4>      | <4>        | <4>      | <4>  |
| An Industrial Strength Audio Search Algorithm                                                  | [[file:~/Projects/Music-Classification/References/An_Industrial_Strength_Audio_Search_Algorithm.pdf][Link]] | X        |            |          |      |
| A Music Similarity Function Based on Signal Analysis                                           | [[file:~/Projects/Music-Classification/References/Music_Similarity_Function_Based_on_Signal_Analysis.pdf][Link]] | X        |            |          |      |
| Signal Processing for Music Analysis                                                           | [[file:~/Projects/Music-Classification/References/Signal_Processing_for_Music_Analysis.pdf][Link]] | X        |            |          |      |
| Music Similarity Measures - What's the Use                                                     | [[file:~/Projects/Music-Classification/References/Music-similarity-measures-whats-the-use.pdf][Link]] | X        |            |          |      |
| Find Similar Music Using FFT Spectrums (StackExchange)                                         | [[https://dsp.stackexchange.com/questions/1370/find-similar-music-using-fft-spectrums][Link]] | X        | X          | X        | X    |
| Shazam Music Processing: Fingerprinting and Recognition (Website)                              | [[https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition][Link]] | X        |            |          |      |
| Contextual Personalized Re-Ranking of Music Recommendations through Audio Features             | [[file:~/Projects/Music-Classification/References/contextualized-personal-reranking-of-music.pdf][Link]] | X        |            |          |      |
| Computational Models of Music Similarity and their Applications in Music Information Retrieval | [[file:~/Projects/Music-Classification/References/Computational-Models-of-Music-Similarity-and-their-Application-in-Music-Information-Retrieval.pdf][Link]] | X        |            |          |      |
| Beat Tracking by Dynamic Programming                                                           | [[file:~/Projects/Music-Classification/References/beat-tracking-by-dynamic-programming.pdf][Link]] | X        |            |          |      |

** An Industrial Strength Audio Search Algorithm
*Abstract*
Flexible audio search engine. Noise and distortion resistant, compute efficient, scalable.
Takes snippet of sound, and identifies it against database of "millions" of tracks.
/Detail/: Combinatorially hashed time-frequency constellation
** A Music Similarity Function Based on Signal Analysis
*Abstract*
Method to compare songs on "audio content".
Creates signature for each song based on "K-means" clustering of "special features".
Handles simple corruption well.
Method is about 50% effective.
** Signal Processing for Music Analysis
*Abstract*
Overview of signal analysis techniques to address dimensions like harmony, melody, rhythym, timbre.
Impact of certain characteristics. Implications of these techniques. 
** Music Similarity Measures - What's the Use
*Abstract*
Measures to compare music titles based on timbre.
Timbre extractor and timbral similarity relation.
Describes experiments to measure quality of relations.
** Find Similar Music Using FFT Spectrums (StackExchange)
*Initial Post*
Attempt to match similar songs in large library.
Attempts to use FFT to calculate spectrum of a song, comparing spectra to determine similarity.
Seeks advice.

*First Read*
Commenter believes that, since music is recorded with the goal of maximizing spectral spread, a full spectrum may not be effective.
Suggests looking into a spectrogram.

There are many dimensions in which to measure music (timbre/texture/genre, rhythmic pattern, melody/chord progression ... etc).
Comparing these dimensions will find similarities in different ways, and produce different results.
*Mel Frequency Cepstrum Coefficients* will help analyze timbre/texture/genre
- "Somehow" capture the way human hearing works (frequency warping, log scale)
*Fluctuation Patterns* or *"Penny" features* will help analyze rhythmic pattern
- fluctuation patterns - Pampalk(?), "autocorrelation of the signal in the 0.1-10 Hz range over a few bands"
- Whitman's "Penny" features - FFT of the MFCC along the time axis
*Chromagrams* will help analyze melody and chord progression
- Start with "Ellis" chromagram code - [[http://labrosa.ee.columbia.edu/matlab/chroma-ansyn/][Link]]
- Mauch's implementation (more robust?) - [[http://isophonics.net/nnls-chroma][Link]]

The methods above help represent music as a sequence of features.
Will have to find a method for comparing two such sequences.
/Note/ - As LT mentioned, comparing these sequences pairwise is not smart - if there is a second or two delay in one of two otherwise identical tracks, the two sequences will be different. Should instead compare the /Distribution/ of the two sequences.
One method - computing the mean and standard deviation of the features over song A, then over song B, and taking the probalistic distance between the two (mentioned KL, Bhattacharyya)

Mentions that it will be relatively ineffective to find the distance between a single song and the rest of the music collection.
Suggests using something like "LSH" or "Ball trees" - allows queries to be performed on near neighbors without explicit comparison with the entire library.
** Shazam Music Processing: Fingerprinting and Recognition (Website)
*Abstract*
Describes how Shazam works.
Walks through code implementation for identifying a song based on a snippet.

*First Read*
Defines the basic process of capturing an  analog sound signal to a digital representation.
Human hearing range is 20 Hz to 20,000 Hz, so we should sample at a rate of 44,100 Hz.

Describes general process for reading a song in code. Uses java, but process is translatable to python.
This data is currently stored in time-domain. Use the DFT (FFT really) to transform the list of time domain samples into a finite combination of complex sinusoids, ordered by frequency.
Interestingly, the implementation of the FFT that this article describes chooses to break the signal into even and odd indices, perform the FFT on the even and odd components seperately, then combine.
They choose to do this because they appear to be following something known as the "Cooley-Turkey Algorithm". This is simply a method to simplify computation.

A downside of the FFT that the article points out is the lack of context. The FFT tells you information about every frequency that appears in a signal, but it doesn't tell you anything about /when/ in that signal those frequencies occur.
The article proposes implementing a sliding window, and only performing the FFT on that window.This will result in an array of arrays instead of the typical single complex array that the FFT outputs.
From this array, we can start to form a "fingerprint".

Our biggest issue at this stage is that we have /too much/ information. In order to reduce the complexity of the operations we need to perform, we can pare down the number of signals by focusing on specific "bins" where the most significant frequencies will reside. One way to divide our frequencies is to identify "bins" of specific frequency ranges, then within each bin, identify the individual frequency with the highest amplitude.
This article chooses to hash the frequency results, which is apparently useful for data compression.
The process essentially looks like this:
 Window 1 -- Bin 1 -- Max Freq -- Hash
 Window 1 -- Bin 2 -- Max Freq -- Hash
 ...
 Window 1 -- Concatenate Hashes
 Window 2 -- Bin 1 -- Max Freq -- Hash
...
Window 2 -- Concatenate Hashes
The result should be an array of hashes.

When recording audio, it's important to note that the recording is subject to noise and disruption. In order to account for this, we include something called a "fuzz factor", which is a sort of tolerance for noise.

In order to identify a song, we must frst construct a database. This data base should not correlate the different "window" sections of a song - in order to allow the system to identify a song based on theoretically "any" portion of the song.
The database will look something like this:
| Hash              | time (seconds) | Song ID            |
| 30 51 99 121 195  |          53.52 | Song A by Artist A |
| 33 56 92 151 185  |          12.32 | Song B by Artist B |
| 39 26 89 141 251  |          15.34 | Song C by Artist C |
| 32 67 100 128 195 |          78.43 | Song D by Artist D |
| 34 57 95 111 200  |          54.52 | Song A by Artist A |

Here, this table contains tag values for different portions of songs. There are two entries for Song A by Artist A, because they represent different windows of the same song. The hash is the "key" for the table, that is, when the system starts to analyze a new snippet of a song, it will find the hash of the newly recorded snippet and compare it to those already stored in the database. When it finds a match, that is how it will associate a name to the snipped of audio.

It's relatively common for a song to use motifs and riffs from other pieces however, which can mean that a search for a single hash will produce several matches. This is what the second column, time, is for. This is somewhat complicated - we can' match the exact time that a hash corresponds to in the song, because the same motif might happen several times throughout the recording. Instead, we compare relative timings. If we match two hash sequences, we can compare their relative timings, which enables us to differentiate between two songs with similar themes.

*** Extracted Code
:PROPERTIES:
:VISIBILITY: folded
:END:

Returns an 'AudioFormat' object with specific settings for audio data - sample rate, sample size, number of channels.
#+begin_src java
private AudioFormat getFormat() {
    float sampleRate = 44100;
    int sampleSizeInBits = 16;
    int channels = 1;          //mono
    boolean signed = true;     //Indicates whether the data is signed or unsigned
    boolean bigEndian = true;  //Indicates whether the audio data is stored in big-endian or little-endian order
    return new AudioFormat(sampleRate, sampleSizeInBits, channels, signed, bigEndian);
}

final AudioFormat format = getFormat(); //Fill AudioFormat with the settings
DataLine.Info info = new DataLine.Info(TargetDataLine.class, format);
final TargetDataLine line = (TargetDataLine) AudioSystem.getLine(info);
line.open(format);
line.start();
#+end_src

Opens 'TargetDataLine' to capture audio data based on format. Reads data from line in a loop, writes to 'OutputStream'
#+begin_src java
OutputStream out = new ByteArrayOutputStream();
running = true;

try {
    while (running) {
        int count = line.read(buffer, 0, buffer.length);
        if (count > 0) {
            out.write(buffer, 0, count);
        }
    }
    out.close();
} catch (IOException e) {
    System.err.println("I/O problems: " + e);
    System.exit(-1);
}
#+end_src

Implements Cooley-Turkey FFT algorithm to calculate the FFT of an input array of complex numbers.
#+begin_src java
public static Complex[] fft(Complex[] x) {
    int N = x.length;
    
    // fft of even terms
    Complex[] even = new Complex[N / 2];
    for (int k = 0; k < N / 2; k++) {
        even[k] = x[2 * k];
    }
    Complex[] q = fft(even);

    // fft of odd terms
    Complex[] odd = even; // reuse the array
    for (int k = 0; k < N / 2; k++) {
        odd[k] = x[2 * k + 1];
    }
    Complex[] r = fft(odd);

    // combine
    Complex[] y = new Complex[N];
    for (int k = 0; k < N / 2; k++) {
        double kth = -2 * k * Math.PI / N;
        Complex wk = new Complex(Math.cos(kth), Math.sin(kth));
        y[k] = q[k].plus(wk.times(r[k]));
        y[k + N / 2] = q[k].minus(wk.times(r[k]));
    }
    return y;
}
#+end_src

Applies the implemented FFT. Converts captured audio (from 'OutputStream', probably) to an array of complex numbers, and performs the FFT on each chunk of data.
#+begin_src java
byte audio [] = out.toByteArray()
int totalSize = audio.length
int sampledChunkSize = totalSize/chunkSize;
Complex[][] result = ComplexMatrix[sampledChunkSize][];

for(int j = 0;i < sampledChunkSize; j++) {
  Complex[chunkSize] complexArray;

  for(int i = 0; i < chunkSize; i++) {
    complexArray[i] = Complex(audio[(j*chunkSize)+i], 0);
  }

  result[j] = FFT.fft(complexArray);
}

#+end_src

Analyzes FFT results. Finds the highest magnitude within each of a set range of frequencies. Does so on several "windows" of data.
#+begin_src java
public final int[] RANGE = new int[] { 40, 80, 120, 180, 300 };

// find out in which range is frequency
public int getIndex(int freq) {
    int i = 0;
    while (RANGE[i] < freq)
        i++;
    return i;
}
    
// result is complex matrix obtained in previous step
for (int t = 0; t < result.length; t++) {
    for (int freq = 40; freq < 300 ; freq++) {
        // Get the magnitude:
        double mag = Math.log(results[t][freq].abs() + 1);

        // Find out which range we are in:
        int index = getIndex(freq);

        // Save the highest magnitude and corresponding frequency:
        if (mag > highscores[t][index]) {
            points[t][index] = freq;
        }
    }
    
    // form hash tag
    long h = hash(points[t][0], points[t][1], points[t][2], points[t][3]);
}

private static final int FUZ_FACTOR = 2;

private long hash(long p1, long p2, long p3, long p4) {
    return (p4 - (p4 % FUZ_FACTOR)) * 100000000 + (p3 - (p3 % FUZ_FACTOR))
            * 100000 + (p2 - (p2 % FUZ_FACTOR)) * 100
            + (p1 - (p1 % FUZ_FACTOR));
}

#+end_src

Defines a data point class that represents a specific moment in a song, with attributes for song ID and time.
#+begin_src java
// Class that represents specific moment in a song
private class DataPoint {

    private int time;
    private int songId;

    public DataPoint(int songId, int time) {
        this.songId = songId;
        this.time = time;
    }
    
    public int getTime() {
        return time;
    }
    public int getSongId() {
        return songId;
    }
}
#+end_src

** Contextual Personalized Re-Ranking of Music Recommendations through Audio Features
*Abstract*
Presents "contextual" ranking algorithm.
Represents user preferences based on high-level audio features, like tempo and valence.
Global and user specific models. 
** Computational Models of Music Similarity and their Applications in Music Information Retrieval
*Abstract*
Describes computational models of music similarity, and their efficacy.
Goes on to outline applications of these models and techniques.
Describes 3 features:
- organize and visualize music collections - control "similarity aspects"
- Organize collection into overlapping hierarchy based on artist
- Generate playlists with minimal user input.
** Beat Tracking by Dynamic Programming
*Abstract*
Defines beat tracking - sequence of "beat instants" (when a human listener might "tap their foot").
Two constraints - actual tempo of the music, and reflects a locally-constant inter-beat-interval.
Uses dynamic programming to find a match between tempo and moments of high "onset strength". 


* Topics
:PROPERTIES:
:VISIBILITY: folded
:END:
** Spectrogram
** Chromagrams
** Mel Frequency Cepstrum Coefficients (MFCC)
** Fluctuation Patterns
** "Penny" Features
** LSH
** Ball Trees
* Inbox

** TODO Review References [11%]
- [ ] An Industrial Strength Audio Search Algorithm
- [ ] A Music Similarity Function Based on Signal Analysis
- [ ] Signal Processing for Music Analysis
- [ ] Music Similarity Measures - What's the Use
- [X] Find Similar Music Using FFT Spectrums (StackExchange)
- [ ] Shazam Music Processing: Fingerprinting and Recognition (Website)
- [ ] Contextual Personalized Re-Ranking of Music Recommendations through Audio Features
- [ ] Computational Models of Music Similarity and their Applications in Music Information Retrieval
- [ ] Beat Tracking by Dynamic Programming


** TODO Research Spectrogram

** TODO Research Chromagrams

** TODO Research Mel Frequency Cepstrum Coefficients (MFCC)

** TODO Research Fluctuation Patterns, "Penny" Features

** TODO Research LSH, Ball trees

** TODO Look into similarity Metrics
** TODO Review CLustering ALgorithms
Allows you to reduce the amount of data you process by only dealing with the clusters that are relevant to the original song.
** TODO dict.get()
* Weekly Reflection
** Week 1
:PROPERTIES:
:EXPORT_FILE_NAME: /home/csj7701/Projects/Music-Classification/Class-Documents/Week-1-Reflection
:END:
- This week, I focused on familiarizing myself with the scope of our project and beginning to dive into the provided research material. I began reading through each of the papers, and have focused primarily on increasing my understanding of the general topics that we will be working with.
- I have fully read through one of the papers, made my way through the introductory material and about half the content on another, and examined abstract and conclusion on all others. I did not have time for much work past that this week, as I had classwork and several examinations.
- My biggest challenge this week was a general lack of understanding. As I read through the material, I was faced with terms with which I was unfamiliar, and concepts that I had not been introduced to previously. I made an effort to gain a loose understanding for all the concepts in the sections I read, and have compiled a list of topics that I will need to research more in-depth at a later date.
- From my readings, I have determined that there are 3 primary sections to this project - "Characterizing" a song, which will be based on one of several methods, or a combination of these (FFT, MFCC, "Penny" Features, or Chromagrams); Comparing these characterizations, likely using some form of distribution; and Applying those characterizations, which I anticipate as the most difficult portion of the project since it will involve a series of design decisions in order to create a working product.

** Week 2
:PROPERTIES:
:EXPORT_FILE_NAME: /home/csj7701/Projects/Music-Classification/Class-Documents/Week-2-Reflection
:END:
- This week, I focused primarily on getting a semi-working product that would provide recommendations based on a single audio feature. Josh had already written a python implementation of the Shazam Algorithm, so I focused on deciding what from that approach fit our goals.
- I went through several iterations, weeding out decisions that shazam made which did not apply to our project. I removed the hashing algorithm, since hashing a value essentially strips much of the data from it. I chose not to implement the sliding window that shazam uses, because we likely don't care about timing information. I was left with an FFT that divided the X-axis into segments, taking the bin with the highest magnitude from each segment. After several misconceptions about the information represented by a non-normalized FFT, I found a way to segment the FFT by frequency.
- My current approach is to take the FFT of a signal, segment that signal based on frequency breakpoints, then identify the bin with the highest magnitude in each segment. For each bin, I get a complex number. I save these complex numbers as an array, and use the arrays to compare songs. In order to compare, I find the linear distance between the complex numbers for Segment 1 of Song1 and Song2, the for Segment 2, and so on. I average those distances to find a single number. In order to sort songs, I use a for loop and write song names and distances to a dataframe.
- This process has helped me gain a better understanding for how to examine and apply the information that the FFT represents. 


**  Week 3
:PROPERTIES:
:EXPORT_FILE_NAME: /home/csj7701/Projects/Music-Classification/Class-Documents/Week-3-Reflection
:END:
- This week I focused primarily on reviewing the work that we have done so far. In order to make sure that we are as prepared for the draft paper due Wednesday, I have been focusing on making sure that we had a general understanding for the implementation of the FFT that we use, and the other design decisions we made.
- I also finished implementing our audio player, so that we now have a CLI option to play the music recommendations we produce.
- Through this research, I have gotten a much better understanding for how Numpy actually implements the FFT, and I think I have a more robust comprehension for how these concepts are applied to actual analysis of sound data.
- The majority of this week I spent studying for the DSP exam on Friday, and reviewing work in preparation for exams in other classes. I only spent about 5 hours this week on the project, but I feel that those 5 hours were well spent, especially given how far along we already are.
